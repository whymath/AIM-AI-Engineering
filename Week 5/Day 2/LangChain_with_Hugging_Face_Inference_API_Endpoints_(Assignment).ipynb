{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctrwj6Cj24Zp"
      },
      "source": [
        "# LangChain with Open Source LLM and Open Source Embeddings & LangSmith\n",
        "\n",
        "In the following notebook we will dive into the world of Open Source models hosted on Hugging Face's [inference endpoints](https://ui.endpoints.huggingface.co/).\n",
        "\n",
        "The notebook will be broken into the following parts:\n",
        "\n",
        "- ðŸ¤ Breakout Room #1:\n",
        "  1. Set-up Hugging Face Infrence Endpoints\n",
        "  2. Install required libraries\n",
        "  3. Set Environment Variables\n",
        "  4. Testing our Hugging Face Inference Endpoint\n",
        "  5. Creating LangChain components powered by the endpoints\n",
        "  6. Retrieving data from Arxiv\n",
        "  7. Creating a simple RAG pipeline with [LangChain v0.1.0](https://blog.langchain.dev/langchain-v0-1-0/)\n",
        "  \n",
        "\n",
        "- ðŸ¤ Breakout Room #2:\n",
        "  1. Set-up LangSmith\n",
        "  2. Creating a LangSmith dataset\n",
        "  3. Creating a custom evaluator\n",
        "  4. Initializing our evaluator config\n",
        "  5. Evaluating our RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AduTna3oCbP4"
      },
      "source": [
        "# ðŸ¤ Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENUY6OSnDy7A"
      },
      "source": [
        "## Task 1: Set-up Hugging Face Infrence Endpoints\n",
        "\n",
        "Please follow the instructions provided [here](https://github.com/AI-Maker-Space/AI-Engineering/tree/main/Week%205/Thursday) to set-up your Hugging Face inference endpoints for both your LLM and your Embedding Models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-spIWt2J3Quk"
      },
      "source": [
        "## Task 2: Install required libraries\n",
        "\n",
        "Now we've got to get our required libraries!\n",
        "\n",
        "We'll start with our `langchain` and `huggingface` dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwGLnp31jXJj",
        "outputId": "894efab9-f2d7-4f2b-fe9a-b9d57152eace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (0.1.17)\n",
            "Requirement already satisfied: langchain-core in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (0.1.52)\n",
            "Requirement already satisfied: langchain-community in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (0.0.37)\n",
            "Requirement already satisfied: langchain_openai in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (0.1.6)\n",
            "Requirement already satisfied: huggingface-hub in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (0.23.0)\n",
            "Requirement already satisfied: requests in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (2.31.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (0.6.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (0.1.52)\n",
            "Requirement already satisfied: numpy<2,>=1 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (1.26.3)\n",
            "Requirement already satisfied: pydantic<3,>=1 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain-core) (23.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.24.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain_openai) (1.24.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.5.2 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain_openai) (0.6.0)\n",
            "Requirement already satisfied: filelock in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from huggingface-hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from huggingface-hub) (2024.2.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from huggingface-hub) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from huggingface-hub) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from requests) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.3.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2024.4.28)\n",
            "Requirement already satisfied: colorama in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub) (0.4.6)\n",
            "Requirement already satisfied: httpcore==1.* in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain langchain-core langchain-community langchain_openai huggingface-hub requests -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPXElql-EE9Q"
      },
      "source": [
        "Now we can grab some miscellaneous dependencies that will help us power our RAG pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FMJqq8SYt34V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: arxiv in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (2.1.0)\n",
            "Requirement already satisfied: pymupdf in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (1.24.2)\n",
            "Requirement already satisfied: faiss-cpu in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (1.8.0)\n",
            "Requirement already satisfied: feedparser==6.0.10 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from arxiv) (6.0.10)\n",
            "Requirement already satisfied: requests==2.31.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from arxiv) (2.31.0)\n",
            "Requirement already satisfied: sgmllib3k in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from requests==2.31.0->arxiv) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from requests==2.31.0->arxiv) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from requests==2.31.0->arxiv) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from requests==2.31.0->arxiv) (2024.2.2)\n",
            "Requirement already satisfied: PyMuPDFb==1.24.1 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from pymupdf) (1.24.1)\n",
            "Requirement already satisfied: numpy in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from faiss-cpu) (1.26.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install arxiv pymupdf faiss-cpu -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpZTBLwK3TIz"
      },
      "source": [
        "## Task 3: Set Environment Variables\n",
        "\n",
        "We'll need to set our `HF_TOKEN` so that we can send requests to our protected API endpoint.\n",
        "\n",
        "We'll also set-up our OpenAI API key, which we'll leverage later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NspG8I0XlFTt",
        "outputId": "d56a1a4e-4175-4522-9100-2261f412efbb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HuggingFace Write Token: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giMejsXN7EKb",
        "outputId": "23327edb-4f59-4d6c-e145-54721b0836ad"
      },
      "outputs": [],
      "source": [
        "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3M7TzXs3WsJ"
      },
      "source": [
        "## Task 4: Testing our Hugging Face Inference Endpoint\n",
        "\n",
        "Let's submit a sample request to the Hugging Face Inference endpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uyFgZVUSEexW"
      },
      "outputs": [],
      "source": [
        "model_api_gateway = \"https://jru0lioakhbitfak.us-east-1.aws.endpoints.huggingface.cloud\" # << YOUR ENDPOINT URL HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvnMlmEsEiqS"
      },
      "source": [
        "> NOTE: If you're running into issues finding your API URL you can find it at [this](https://ui.endpoints.huggingface.co/) link.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "![image](https://i.imgur.com/XyZhOv8.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fVaR1onmtkz",
        "outputId": "136a97cf-8faa-45de-fe1e-2c1449971d83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': \"Hello! How are you? I'm doing well, thanks for asking! *smiles* It's great to see you here! *nods* Is there anything new you'd like to talk about or ask? I'm all ears! *winks*\"}]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "max_new_tokens = 256\n",
        "top_p = 0.9\n",
        "temperature = 0.1\n",
        "\n",
        "prompt = \"Hello! How are you?\"\n",
        "\n",
        "json_body = {\n",
        "    \"inputs\" : prompt,\n",
        "    \"parameters\" : {\n",
        "        \"max_new_tokens\" : max_new_tokens,\n",
        "        \"top_p\" : top_p,\n",
        "        \"temperature\" : temperature\n",
        "    }\n",
        "}\n",
        "\n",
        "headers = {\n",
        "  \"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\",\n",
        "  \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(model_api_gateway, json=json_body, headers=headers)\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXTBnBTy3b62"
      },
      "source": [
        "## Task 5: Creating LangChain components powered by the endpoints\n",
        "\n",
        "We're going to wrap our endpoints in LangChain components in order to leverage them, thanks to LCEL, as we would any other LCEL component!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd5DaxGEFohF"
      },
      "source": [
        "### HuggingFaceEndpoint for LLM\n",
        "\n",
        "We can use the `HuggingFaceEndpoint` found [here](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/huggingface_endpoint.py) to power our chain - let's look at how we would implement it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vc7K1rFhSVt",
        "outputId": "73f7b340-5156-4f6b-fd1f-11e588b49337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to C:\\Users\\ymath\\.cache\\huggingface\\token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "endpoint_url = (\n",
        "    model_api_gateway\n",
        ")\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        "    task=\"text-generation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-PBb3MPFN_t"
      },
      "source": [
        "Now we can use our endpoint like we would any other LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "mMJrWnKISFqb",
        "outputId": "d661643a-b611-4766-a84e-55afeb473d91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" I am here to help you with any questions you may have. We are a team of experienced professionals who are passionate about providing excellent customer service. Please feel free to ask us anything, and we will do our best to assist you.\\n\\nWe understand that you may have questions or concerns about our products or services, and we are here to address them. Whether you need help with an order, have a question about a product, or want to provide feedback, we are here to listen and help.\\n\\nWe value your feedback and appreciate your business. We are committed to providing you with the best possible experience, and we are always looking for ways to improve. Please don't hesitate to reach out to us if you have any questions or concerns.\\n\\nThank you for choosing to shop with us. We look forward to serving you in the future.\\n\\nBest regards,\\n\\n[Your Name]\\n[Your Company]\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hf_llm.invoke(\"Hello, how are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EBtSBMj3-Hu"
      },
      "source": [
        "### HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "Now we can leverage the `HuggingFaceInferenceAPIEmbeddings` module in LangChain to connect to our Hugging Face Inference Endpoint hosted embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wrZJHVGkGLZr"
      },
      "outputs": [],
      "source": [
        "embedding_api_gateway = \"https://akzntviirezn1nba.us-east-1.aws.endpoints.huggingface.cloud\" # << Embedding Endpoint API URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4asz9Ofn0MtP"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "embeddings_model = HuggingFaceInferenceAPIEmbeddings(api_key=os.environ[\"HF_TOKEN\"], api_url=embedding_api_gateway)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvF_eMZZKnlm",
        "outputId": "56958042-8212-406f-b378-ed018f5ffad2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-0.026424622,\n",
              " 0.035885748,\n",
              " 0.0094334055,\n",
              " 0.011660095,\n",
              " 0.0065645785,\n",
              " 0.008227667,\n",
              " -0.036902077,\n",
              " -0.03631076,\n",
              " -0.024853928,\n",
              " -0.005395797]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_model.embed_query(\"Hello, welcome to HF Endpoint Embeddings\")[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtbNzDF-e7JI"
      },
      "source": [
        "#### â“ Question #1\n",
        "\n",
        "What is the embedding dimension of your selected embeddings model?\n",
        "\n",
        "#### ANSWER:\n",
        "\n",
        "We're using the `snowflake-arctic-embed-m` embedding model, and it has an embedding dimension of `768` as can be seen [here](https://huggingface.co/Snowflake/snowflake-arctic-embed-m)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9pLgHfR3uY9"
      },
      "source": [
        "## Task 6: Retrieving data from Arxiv\n",
        "\n",
        "We'll leverage the `ArxivLoader` to load some papers about the \"QLoRA\" topic, and then split them into more manageable chunks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7yO05R6mtyCB"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "docs = ArxivLoader(query=\"QLoRA\", load_max_docs=5).load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4F249yWeuCKd"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 250,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9BO1Y1Xur0e",
        "outputId": "3b637172-2b49-4f51-fb02-77f7609e677b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1305"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sZBBjdM4Or8"
      },
      "source": [
        "Just the same as we would with OpenAI's embeddings model - we can instantiate our `FAISS` vector store with our documents and our `HuggingFaceEmbeddings` model!\n",
        "\n",
        "We'll need to take a few extra steps, though, due to a few limitations of the endpoint/FAISS.\n",
        "\n",
        "We'll start by embeddings our documents in batches of `32`.\n",
        "\n",
        "> NOTE: This process might take a while depending on the compute you assigned your embedding endpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FBCTm-JZ0mVr"
      },
      "outputs": [],
      "source": [
        "embeddings = []\n",
        "for i in range(0, len(split_chunks) - 1, 32):\n",
        "  embeddings.append(embeddings_model.embed_documents([document.page_content for document in split_chunks[i:i+32]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4wLY8FDGNDym"
      },
      "outputs": [],
      "source": [
        "embeddings = [item for sub_list in embeddings for item in sub_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgc_e-9QHJTm"
      },
      "source": [
        "#### â“ Question #2\n",
        "\n",
        "Why do we have to limit our batches when sending to the Hugging Face endpoints?\n",
        "\n",
        "#### ANSWER:\n",
        "\n",
        "While running batch inference, limiting batch size might be needed for memory usage reasons. Transferring data to and from a GPU can be a bottleneck, so it is generally advisable to ensure that the endpoint is able to fit the embedding inference of the batch's data in the GPU VRAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn4lECg2TTza"
      },
      "source": [
        "Now we can create text/embedding pairs which we want use to set-up our FAISS VectorStore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6C1bw7srOVJX"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "text_embedding_pairs = list(zip([document.page_content for document in split_chunks], embeddings))\n",
        "\n",
        "faiss_vectorstore = FAISS.from_embeddings(text_embedding_pairs, embeddings_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXbexmFSTZKF"
      },
      "source": [
        "Next, we set up FAISS as a retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BSUZYfvAPxTF"
      },
      "outputs": [],
      "source": [
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\" : 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce1ZWj8aTchK"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DwHoaIDQQ9E",
        "outputId": "bfdc1130-e26f-49c1-cbc1-79a9389a8cbd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Workspaces\\Courses\\AIMakerspace\\AI-Engineering-Cohort-2\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(page_content='We have discussed how QLoRA works and how it can significantly reduce the required memory for\\nfinetuning models. The main question now is whether QLoRA can perform as well as full-model'),\n",
              " Document(page_content='of QDyLoRA through several instruct-fine-tuning\\nTable 3: Comparing the performance of DyLoRA, QLoRA and QDyLoRA across different evaluation ranks. all'),\n",
              " Document(page_content='QLoRA delivers convincing accuracy improvements across\\nthe LLaMA and LLaMA2 families, even with 2-4 bit-widths,\\naccompanied by a minimal 0.45% increase in time con-\\nsumption. Remarkably versatile, IR-QLoRA seamlessly'),\n",
              " Document(page_content='performance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\\na pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [28]\\nâˆ—Equal contribution.'),\n",
              " Document(page_content='Moreover, QLoRA is trained\\non a pre-defined rank and, therefore, cannot\\nbe reconfigured for its lower ranks without\\nrequiring further fine-tuning steps. This pa-\\nper proposes QDyLoRA -Quantized Dynamic\\nLow-Rank Adaptation-, as an efficient quantiza-')]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "faiss_retriever.get_relevant_documents(\"What optimizer does QLoRA use?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm0IjkpFSdmw"
      },
      "source": [
        "### Prompt Template\n",
        "\n",
        "Now that we have our LLM and our Retiever set-up, let's connect them with our Prompt Template!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Gqpayd-kTyiq"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NikHqHljIIdK"
      },
      "source": [
        "#### â“ Question #3\n",
        "\n",
        "Does the ordering of the prompt matter?\n",
        "\n",
        "#### ANSWER:\n",
        "\n",
        "For most types of general usage RAG prompts, as long as the entire prompt fits within the context window, the ordering of the prompt components ('instruction', 'context', 'question') shouldn't matter too much, especially with capable models like GPT-3.5 and GPT-4.\n",
        "\n",
        "However as a general rule of thumb, it might be best to have the 'instruction' with the task description at the beginning to help optimally 'prime' (aka optimus prime &#128516;) the model for the specific task. That would help to set the expectation for the response from the beginning of the provided context onwards.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwy1YOy34aXf"
      },
      "source": [
        "## Task 7: Creating a simple RAG pipeline with LangChain v0.1.0\n",
        "\n",
        "All that's left to do is set up a RAG chain - and away we go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "i0q8CUu809M-"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | faiss_retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | rag_prompt\n",
        "    | hf_llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHyy5p484iUD"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OezUhZGrUr63",
        "outputId": "c60a6f33-31a5-4b44-dbeb-647a7a94e1f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLORA is a method for fine-tuning high-quality LLMs (Language Model) in a more accessible and efficient way. It has the potential to significantly reduce the required memory for finetuning models, making it easier to use for a wider range of applications. QLORA also has the ability to perform as well as full-model fine-tuning, and has been shown to enhance the capabilities of IR-QLoRA in intricate understanding and text generation quality.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLORA all about?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGsV8x_ZIWZ9"
      },
      "source": [
        "# ðŸ¤ Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrKQSs_r4gl8"
      },
      "source": [
        "## Task 1: Set-up LangSmith\n",
        "\n",
        "We'll be moving through this notebook to explain what visibility tools can do to help us!\n",
        "\n",
        "Technically, all we need to do is set-up the next cell's environment variables!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S5X3EE847PO",
        "outputId": "2cec0c3e-10c2-4c34-b7c6-21fadd39a2e7"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou1fLN-MJGfu"
      },
      "source": [
        "Let's see what happens on the LangSmith project when we run this chain now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "1Yr8j5hqJGET",
        "outputId": "1108b929-4e98-4f85-8c68-7e5e5368e54e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLoRA is a method for reducing the memory requirements for finetuning pre-trained language models (LLMs). It allows for the finetuning of high-quality LLMs much more widely and easily accessible, and has the potential for future work via QLORA tuning on specialized open-source data, which produces models that can compete with the very best commercial models that exist today. We have discussed how QLoRA works and how it can significantly reduce the required memory for finetuning models, and the main question now is whether QLoRA can perform as well as full-model fine-tuning. Our qualitative assessment accentuates the enhanced capabilities of IR-QLoRA in intricate understanding and text generation quality, compared to the QLoRA baseline.'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA all about?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmaxEfcWJWXc"
      },
      "source": [
        "We get *all of this information* for \"free\":\n",
        "\n",
        "![image](https://i.imgur.com/8Wcpmcj.png)\n",
        "\n",
        "> NOTE: We'll walk through this diagram in detail in class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsFaAg1TJ8JE"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #1:\n",
        "\n",
        "Please describe the trace of the previous request and answer these questions:\n",
        "\n",
        "1. How many tokens did the request use?\n",
        "2. How long did the `HuggingFaceEndpoint` take to complete?\n",
        "\n",
        "#### ANSWER:\n",
        "\n",
        "The above trace shows the execution of a LangChain RunnableSequence that performs the following sequence of actions:\n",
        "- takes the `{\"question\": \"What is QLoRA?\"}` input dictionary and uses the (FAISS vector store) retreiver to fetch additional context\n",
        "- generates the templated prompt\n",
        "- returns the text generation output from the HuggingFace inference endpoint\n",
        "\n",
        "Answers to the questions below:\n",
        "1. The token used 299 tokens total\n",
        "2. The `HuggingFaceEndpoint` took 4.71s for completion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XdbE0m3JgJp"
      },
      "source": [
        "## Task 2: Creating a LangSmith dataset\n",
        "\n",
        "Now that we've got LangSmith set-up - let's explore how we can create a dataset!\n",
        "\n",
        "First, we'll create a list of questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-KVSO6Eh5DpC"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLbc0B8K6QZ"
      },
      "source": [
        "Now we can create our dataset through the LangSmith `Client()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "NUH0m7AuKyn7"
      },
      "outputs": [],
      "source": [
        "client = Client()\n",
        "# dataset_name = \"QLoRA RAG Dataset v2\"\n",
        "dataset_name = \"QLoRA RAG Dataset v2\" + \"-\" + unique_id\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    dataset_id=dataset.id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jxaByg9LFfX"
      },
      "source": [
        "After this step you should be able to navigate to the following dataset in the LangSmith web UI.\n",
        "\n",
        "![image](https://i.imgur.com/CdFYGTB.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbVQaJi3LsdU"
      },
      "source": [
        "## Task 3: Creating a custom evaluator\n",
        "\n",
        "Now that we have a dataset - we can start thinking about evaluation.\n",
        "\n",
        "We're going to make a `StringEvaluator` to measure \"dopeness\".\n",
        "\n",
        "> NOTE: While this is a fun toy example - this can be extended to practically any use-case!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qofRv8FI7TeZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Any, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.evaluation import StringEvaluator\n",
        "\n",
        "class DopenessEvaluator(StringEvaluator):\n",
        "    \"\"\"An LLM-based dopeness evaluator.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "        template = \"\"\"On a scale from 0 to 100, how dope (cool, awesome, lit) is the following response to the input:\n",
        "        --------\n",
        "        INPUT: {input}\n",
        "        --------\n",
        "        OUTPUT: {prediction}\n",
        "        --------\n",
        "        Reason step by step about why the score is appropriate, then print the score at the end. At the end, repeat that score alone on a new line.\"\"\"\n",
        "\n",
        "        self.eval_chain = PromptTemplate.from_template(template) | llm\n",
        "\n",
        "    @property\n",
        "    def requires_input(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def requires_reference(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def evaluation_name(self) -> str:\n",
        "        return \"scored_dopeness\"\n",
        "\n",
        "    def _evaluate_strings(\n",
        "        self,\n",
        "        prediction: str,\n",
        "        input: Optional[str] = None,\n",
        "        reference: Optional[str] = None,\n",
        "        **kwargs: Any\n",
        "    ) -> dict:\n",
        "        evaluator_result = self.eval_chain.invoke(\n",
        "            {\"input\": input, \"prediction\": prediction}, kwargs\n",
        "        )\n",
        "        reasoning, score = evaluator_result.content.split(\"\\n\", maxsplit=1)\n",
        "        score = re.search(r\"\\d+\", score).group(0)\n",
        "        if score is not None:\n",
        "            score = float(score.strip()) / 100.0\n",
        "        return {\"score\": score, \"reasoning\": reasoning.strip()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PoETszTMSNW"
      },
      "source": [
        "## Task 4: Initializing our evaluator config\n",
        "\n",
        "Now we can initialize our `RunEvalConfig` which we can use to evaluate our chain against our dataset.\n",
        "\n",
        "> NOTE: Check out the [documentation](https://docs.smith.langchain.com/evaluation/faq/custom-evaluators) for adding additional custom evaluators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "pc0bedbe-S2z"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[DopenessEvaluator()],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
        "        RunEvalConfig.Criteria(\n",
        "            {\n",
        "                \"AI\": \"Does the response feel AI generated?\"\n",
        "                \"Response Y if they do, and N if they don't.\"\n",
        "            }\n",
        "        ),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XalvsOjMvdK"
      },
      "source": [
        "## Task 5: Evaluating our RAG pipeline\n",
        "\n",
        "All that's left to do now is evaluate our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6syFWlaF-olk",
        "outputId": "6b724916-154a-4d71-c161-8c4e186f46f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'HF RAG Pipeline - Evaluation - v1-3c467e3e' at:\n",
            "https://smith.langchain.com/o/7fd07b5c-2cc4-5fa5-998d-744414e86be0/datasets/6966008b-2c4a-48b2-bb2e-d701ec992085/compare?selectedSessions=dd3325bf-4806-4bed-9606-79bbc26987b3\n",
            "\n",
            "View all tests for Dataset QLoRA RAG Dataset v2-3c467e3e at:\n",
            "https://smith.langchain.com/o/7fd07b5c-2cc4-5fa5-998d-744414e86be0/datasets/6966008b-2c4a-48b2-bb2e-d701ec992085\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.harmfulness</th>\n",
              "      <th>feedback.AI</th>\n",
              "      <th>feedback.scored_dopeness</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>87379ea3-ed58-46f1-982a-3651feb582a7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.366667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.219102</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.516398</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.516398</td>\n",
              "      <td>0.376386</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.709612</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.212932</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.923500</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.720182</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.149093</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12.044646</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        feedback.helpfulness  feedback.harmfulness  feedback.AI  \\\n",
              "count               6.000000                   6.0     6.000000   \n",
              "unique                   NaN                   NaN          NaN   \n",
              "top                      NaN                   NaN          NaN   \n",
              "freq                     NaN                   NaN          NaN   \n",
              "mean                0.333333                   0.0     0.333333   \n",
              "std                 0.516398                   0.0     0.516398   \n",
              "min                 0.000000                   0.0     0.000000   \n",
              "25%                 0.000000                   0.0     0.000000   \n",
              "50%                 0.000000                   0.0     0.000000   \n",
              "75%                 0.750000                   0.0     0.750000   \n",
              "max                 1.000000                   0.0     1.000000   \n",
              "\n",
              "        feedback.scored_dopeness error  execution_time  \\\n",
              "count                   6.000000     0        6.000000   \n",
              "unique                       NaN     0             NaN   \n",
              "top                          NaN   NaN             NaN   \n",
              "freq                         NaN   NaN             NaN   \n",
              "mean                    0.366667   NaN        5.219102   \n",
              "std                     0.376386   NaN        3.709612   \n",
              "min                     0.100000   NaN        2.212932   \n",
              "25%                     0.100000   NaN        2.923500   \n",
              "50%                     0.150000   NaN        3.720182   \n",
              "75%                     0.687500   NaN        6.149093   \n",
              "max                     0.850000   NaN       12.044646   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      6  \n",
              "unique                                     6  \n",
              "top     87379ea3-ed58-46f1-982a-3651feb582a7  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'HF RAG Pipeline - Evaluation - v1-3c467e3e',\n",
              " 'results': {'626a5572-4e26-44c8-9d64-61638fbeebb6': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the answer provided is \"I don\\'t know.\" This answer is not helpful or insightful as it does not provide any information about the data type created in the QLoRA paper. It is appropriate in the sense that it is a valid response to a question, but it does not meet the criteria of being helpful or insightful. \\n\\nTherefore, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('48229fba-3814-4e2a-810f-de15005e8b18'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is about whether the submission is harmful, offensive, or inappropriate. The submission is \"I don\\'t know.\" This statement is not harmful, offensive, or inappropriate. It is simply an admission of lack of knowledge on the topic. Therefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('85f2a404-ef73-464b-bdb9-0636e83ccb50'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The response is \"I don\\'t know.\" This is a simple, human-like response that could easily be given by a person who is not familiar with the topic. It does not exhibit any characteristics that are typically associated with AI-generated responses, such as overly complex language, lack of context, or inability to understand the question. Therefore, it does not feel AI-generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e744dbdb-fe13-429c-98b4-e9895741b0cb'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment=\"The response is not very dope because it doesn't provide any useful information. The purpose of the question is to get a specific answer about the QLoRA paper, but the response fails to deliver that. It doesn't show any knowledge or understanding of the topic, and it doesn't make any effort to be helpful or informative. It's not funny or clever, and it doesn't add anything to the conversation. It's just a simple, straightforward admission of ignorance, which isn't very dope at all.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c145ad29-a8eb-4aff-94ce-fa3b61241e2c'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 2.965921,\n",
              "   'run_id': '87379ea3-ed58-46f1-982a-3651feb582a7',\n",
              "   'output': \"\\nAnswer:\\nI don't know.\"},\n",
              "  '07b6c81f-3e60-4d2f-996f-a2dcd4b9b471': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is the helpfulness of the submission. \\n\\nThe submission is an answer to the question \"What is a Retrieval Augmented Generation system?\" \\n\\nThe response provided by the submission does not provide any information about the Retrieval Augmented Generation system. Instead, it states that the term is not familiar and cannot provide a clear answer. \\n\\nThis does not meet the criterion of being helpful or insightful as it does not provide any useful information to the question asked. \\n\\nTherefore, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('aac850bd-8c9e-49eb-9afb-ed7bb2723538'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a polite response stating that the system is not familiar with the term \"Retrieval Augmented Generation system.\" There is no harmful, offensive, or inappropriate content in the response.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('8cd94769-288f-448c-aa6d-b4a13c8c898a'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment='The criterion is to assess whether the response feels like it was generated by an AI. The submission does not provide a direct answer to the question, instead it states that it is not familiar with the term and cannot provide a clear answer. This kind of response is typical of an AI system when it does not have the necessary information to generate a relevant answer. Therefore, the submission does meet the criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f698586c-b8da-4eb2-b97a-929a55acc568'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.2, value=None, comment=\"The response is not very dope. The assistant is expected to know about a Retrieval Augmented Generation system, especially since it's a term related to AI and machine learning. The assistant's inability to provide an answer to the question is a significant drawback. However, the assistant's polite and professional tone is a positive aspect of the response. Therefore, the score would be relatively low due to the lack of knowledge but not the lowest possible because of the polite tone.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('5af32f7c-5f50-470a-b4ba-f576e0656cea'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 6.707309,\n",
              "   'run_id': '835be03c-b87f-4ce4-baec-03ebe587716c',\n",
              "   'output': '\\nAnswer:\\nI apologize, but I\\'m not familiar with the term \"Retrieval Augmented Generation system.\" Based on the provided context, it seems that the term is not mentioned in any of the documents. Therefore, I cannot provide a clear answer to your question.'},\n",
              "  '8a63c0ac-cc46-4cf5-999b-80acd685cb4c': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the user has asked who authored the QLoRA paper. The response given is that the author of the QLoRA paper is not mentioned in the provided context. \\n\\nThis response is appropriate because it is true that the author is not mentioned in the provided context. It is also helpful because it informs the user that the information they are looking for is not available in the given context. \\n\\nHowever, it may not be considered insightful because it does not provide any additional information or insight beyond stating that the information is not available. \\n\\nDespite this, the response does meet the overall criterion of being helpful and appropriate, even if it may not be particularly insightful. \\n\\nTherefore, the submission does meet the criteria.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('15815a5e-c611-420b-8255-b68beee206a2'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a polite response stating that the author of the QLoRA paper is not mentioned in the provided context. \\n\\nThere is nothing harmful, offensive, or inappropriate in the response. \\n\\nTherefore, the submission does not meet the criteria of being harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ecf8cadc-59a4-466f-b579-fc8dfcfd92a5'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment=\"The criterion is to assess whether the response feels like it was generated by an AI. The submission is a response to a question about the author of a paper, and the response is that the AI cannot answer the question because the author is not mentioned in the provided context. This response is indicative of an AI's behavior, as an AI would not be able to provide information that is not present in the given context. Therefore, the response does feel AI generated.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('5db92913-b620-4e87-9447-24ea54ed7607'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment='This response is quite good. It is clear, polite, and professional. The AI acknowledges its limitations and explains why it cannot provide the requested information. However, it could be improved by suggesting a way to find the information, such as recommending a search on a scholarly database or the internet. Therefore, I would give it a score of 85 out of 100.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e8a3c8b8-6aac-4c76-ad40-9c59b06a8835'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 4.474444,\n",
              "   'run_id': 'b49d90ab-dd4c-4d04-aaa1-eb047a59ae20',\n",
              "   'output': '\\nAnswer:\\nI apologize, but I cannot answer that question as the author of the QLoRA paper is not mentioned in the provided context.'},\n",
              "  'dd6df01c-87cd-42eb-9ed0-856faf5db9bc': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the answer to the question \"What is the most popular deep learning framework?\" is \"I don\\'t know.\" \\n\\nThis answer is not helpful or insightful as it does not provide any information or insight about the most popular deep learning framework. It is appropriate in the sense that it is a valid response to a question, but it does not meet the criteria of being helpful or insightful. \\n\\nTherefore, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('d65e1ca1-ac08-4e26-bb0c-a23d406b93bc'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is about whether the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is clear that the respondent simply stated \"I don\\'t know.\" This response is not harmful, offensive, or inappropriate. It may not be helpful or informative, but it does not violate the given criterion.\\n\\nTherefore, the submission meets the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('501f4a12-b3ea-4f47-827b-bc8034829834'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The response provided is \"I don\\'t know.\" This is a simple and straightforward response that could be given by both a human and an AI. However, considering that an AI could easily access data to provide a more accurate answer, this response does not necessarily feel AI-generated. It seems more like a human response, as an AI would typically provide a more data-driven answer. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0d3f1ea3-f8f2-4591-b12b-c18d10204cc0'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment=\"The response is not very dope because it doesn't provide any useful information. The question asks for a specific piece of information, and the response fails to deliver it. It's not funny, clever, or insightful, it's just a simple admission of ignorance. While honesty is appreciated, in this context, it's not very cool or awesome.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f39bef48-d153-4ba7-9ff7-37381d24f126'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 2.90936,\n",
              "   'run_id': 'eefa8fce-60f6-410d-ad73-ae53bcfa1f15',\n",
              "   'output': \"\\nAnswer:\\nI don't know.\"},\n",
              "  '28f63953-9a94-40a2-bd3b-bfbfd2894ae9': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nTo assess this, we need to consider whether the submission provides useful and relevant information that answers the question. The question asks for significant improvements made by the LoRA system. \\n\\nThe submission provides two specific improvements made by the LoRA system. It explains how the system reduces the amount of LoRA parameter yields for minor memory benefits, and how it mitigates the performance degradation caused by weight quantization in LLM. \\n\\nThe submission also provides a summary that ties back to the question, stating that the LoRA system makes significant improvements in improving the performance of LLM and mitigating the impact of weight quantization on LLM\\'s performance. \\n\\nTherefore, the submission is helpful and insightful as it provides specific improvements made by the LoRA system and explains how these improvements are significant. \\n\\nThe submission is also appropriate as it directly answers the question and stays on topic. \\n\\nBased on this analysis, the submission meets the criterion of helpfulness. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('244026a0-8f72-4133-bd15-44ab02f624cb'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a technical explanation about the improvements made by the LoRA system. It does not contain any harmful, offensive, or inappropriate content. It is purely informational and related to the task at hand.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1e982124-95c9-41a2-9824-2a939e47abed'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels AI generated. The response provided is coherent, logical, and well-structured. It does not exhibit any of the common signs of AI-generated text, such as nonsensical phrases, abrupt topic changes, or lack of context understanding. The response seems to be written by a human with a good understanding of the LoRA system and its improvements. Therefore, the response does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ad184525-8603-43f4-a0e8-09feb051a9e6'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment=\"The response is quite informative and detailed, providing specific ways in which the LoRA system improves performance. It uses technical language appropriately and accurately, demonstrating a good understanding of the subject matter. However, it could be more engaging and accessible to a wider audience if it included some simpler explanations or analogies. It also doesn't fully explain what the LoRA system is, which could be confusing for someone not familiar with the topic.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ad264a4e-7c70-4ee4-ba9f-71ae68187af8'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 12.044646,\n",
              "   'run_id': 'e1fbb93e-5c1c-4aaa-be94-960b9ee2660d',\n",
              "   'output': \"\\nAnswer:\\nBased on the provided context, the LoRA system makes significant improvements in terms of:\\n\\n1. Reducing the amount of LoRA parameter yields only minor memory benefits, indicating that more adapters can be used to improve performance.\\n2. Mitigating the performance degradation caused by weight quantization in LLM by fine-tuning additional adapters for downstream tasks, sometimes yielding notable performance enhancements.\\n\\nTherefore, the LoRA system makes significant improvements in improving the performance of LLM and mitigating the impact of weight quantization on LLM's performance.\"},\n",
              "  '71ca991d-62f6-4ff8-84c8-7c9b7a5c5575': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the respondent simply states \"I don\\'t know.\" This does not provide any helpful or insightful information about the question asked, which is about the optimizer used in QLoRA. \\n\\nThe response is not inappropriate, but it fails to meet the other aspects of the criteria, namely being helpful and insightful. \\n\\nTherefore, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('edae0c88-1606-4f90-9ac8-c5d3af4e6d3e'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is about whether the submission is harmful, offensive, or inappropriate. The submission is \"I don\\'t know.\" This response is not harmful, offensive, or inappropriate. It simply indicates a lack of knowledge on the part of the respondent. Therefore, the submission does not meet the criterion of being harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('57850f0d-acff-414a-a3fd-af89508d9ca1'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The response is \"I don\\'t know.\" This is a simple, straightforward response that could be given by either a human or an AI. It does not contain any complex language structures or unusual phrasing that might suggest it was generated by an AI. However, it also does not contain any distinctly human elements, such as emotion or personal experience. Therefore, it is not clear whether this response was generated by an AI or not. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('dd7a8349-6d07-4ea5-8bf1-da60a522dcab'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment=\"The response is not very dope because it doesn't provide any useful information to the question asked. The purpose of the question is to gain knowledge about the optimizer used in QLoRA, but the response fails to deliver that. It's not incorrect, but it's not helpful either. Therefore, the score would be low.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('91ed776d-efe0-44e8-83bd-867c4e76c8b7'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 2.212932,\n",
              "   'run_id': '42fd45bd-4ff1-4b6b-9b7f-ea340c9d14e9',\n",
              "   'output': \"\\nAnswer:\\nI don't know.\"}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "project_name = \"HF RAG Pipeline - Evaluation - v1\" + \"-\" + unique_id\n",
        "\n",
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=retrieval_augmented_qa_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    # project_name=\"HF RAG Pipeline - Evaluation - v1\",\n",
        "    project_name=project_name,\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
