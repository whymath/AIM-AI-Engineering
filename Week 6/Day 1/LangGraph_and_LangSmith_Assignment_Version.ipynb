{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "  \n",
        "- ü§ù Breakout Room #2:\n",
        "  - Part 1:\n",
        "    1. Creating an Evaluation Dataset\n",
        "    2. Adding Evaluators\n",
        "    3. Evaluating\n",
        "  - Part 2:\n",
        "    1. Adding conditional check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "d2b0dd31-273b-4f63-b019-50e7eddd15b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (0.1.20)\n",
            "Requirement already satisfied: langchain_openai in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (0.1.6)\n",
            "Requirement already satisfied: langgraph in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (0.0.48)\n",
            "Requirement already satisfied: arxiv in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (2.1.0)\n",
            "Requirement already satisfied: duckduckgo-search in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (5.3.1)\n",
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-6.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (0.6.5)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (0.0.38)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (0.1.52)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (0.1.52)\n",
            "Requirement already satisfied: numpy<2,>=1 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (1.26.3)\n",
            "Requirement already satisfied: pydantic<3,>=1 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (1.10.9)\n",
            "Requirement already satisfied: requests<3,>=2 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.24.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain_openai) (1.24.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.5.2 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain_openai) (0.6.0)\n",
            "Requirement already satisfied: feedparser==6.0.10 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from arxiv) (6.0.10)\n",
            "Requirement already satisfied: sgmllib3k in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: click>=8.1.7 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from duckduckgo-search) (8.1.7)\n",
            "Collecting pyreqwest-impersonate>=0.4.5 (from duckduckgo-search)\n",
            "  Downloading pyreqwest_impersonate-0.4.5-cp38-abi3-win_amd64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: orjson>=3.10.3 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from duckduckgo-search) (3.10.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: colorama in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from click>=8.1.7->duckduckgo-search) (0.4.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (23.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.3.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2024.4.28)\n",
            "Requirement already satisfied: httpcore==1.* in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\workspaces\\courses\\aimakerspace\\ai-engineering-cohort-2\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Downloading duckduckgo_search-6.0.0-py3-none-any.whl (22 kB)\n",
            "Downloading pyreqwest_impersonate-0.4.5-cp38-abi3-win_amd64.whl (2.6 MB)\n",
            "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
            "   ----------- ---------------------------- 0.8/2.6 MB 15.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  2.6/2.6 MB 32.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.6/2.6 MB 27.4 MB/s eta 0:00:00\n",
            "Installing collected packages: pyreqwest-impersonate, duckduckgo-search\n",
            "  Attempting uninstall: duckduckgo-search\n",
            "    Found existing installation: duckduckgo_search 5.3.1\n",
            "    Uninstalling duckduckgo_search-5.3.1:\n",
            "      Successfully uninstalled duckduckgo_search-5.3.1\n",
            "Successfully installed duckduckgo-search-6.0.0 pyreqwest-impersonate-0.4.5\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U langchain langchain_openai langgraph arxiv duckduckgo-search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "db73ce2a-b96e-475c-df1a-374ecf32fdef"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "82647f16-a782-48eb-c0e5-568b8896cdc6"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE2 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "from langchain_community.tools.tavily_search.tool import TavilySearchResults\n",
        "\n",
        "tool_belt = [\n",
        "    # DuckDuckGoSearchRun(),\n",
        "    TavilySearchResults(),\n",
        "    ArxivQueryRun()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/main/langgraph/prebuilt/tool_executor.py) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "#### ANSWER:\n",
        "\n",
        "Both the tools we use extend the `langchain_core.tools.BaseTool` class, so they have a `description` attribute that describes the tool in natural language for the LLM to understand and determine usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "#### ANSWER:\n",
        "\n",
        "There shouldn't be any specific limit to the cycling, outside of practical bounds like the OpenAI API cost/rate limits, local compute resources, etc.\n",
        "\n",
        "To impose a limit on the number of cycles, we could track the length of the `state[\"messages\"]` list in our `should_continue` function, and return \"END\" once it crossed that limit (even before checking for \"function_call\" in the `additional_kwargs`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSCds6zTL5VJ"
      },
      "source": [
        "#### Helper Function to print messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xRPF0X5iL8Bh"
      },
      "outputs": [],
      "source": [
        "def print_messages(messages):\n",
        "  next_is_tool = False\n",
        "  initial_query = True\n",
        "  for message in messages[\"messages\"]:\n",
        "    if \"function_call\" in message.additional_kwargs:\n",
        "      print()\n",
        "      print(f'Tool Call - Name: {message.additional_kwargs[\"function_call\"][\"name\"]} + Query: {message.additional_kwargs[\"function_call\"][\"arguments\"]}')\n",
        "      next_is_tool = True\n",
        "      continue\n",
        "    if next_is_tool:\n",
        "      print(f\"Tool Response: {message.content}\")\n",
        "      next_is_tool = False\n",
        "      continue\n",
        "    if initial_query:\n",
        "      print(f\"Initial Query: {message.content}\")\n",
        "      print()\n",
        "      initial_query = False\n",
        "      continue\n",
        "    print()\n",
        "    print(f\"Agent Response: {message.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "db0f32d3-724a-4f64-9a48-121ee2ddfb0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is RAG in the context of Large Language Models? When did it break onto the scene?\n",
            "\n",
            "\n",
            "Agent Response: RAG stands for Retrieval-Augmented Generation. It is a technique used in Large Language Models to improve the generation of text by combining retrieval-based methods with generative models. RAG models use a retriever to search through a large database of text to find relevant information that can be used to enhance the generation process of the language model.\n",
            "\n",
            "RAG broke onto the scene in 2020 when a research paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" was published by researchers at Facebook AI. This paper introduced the concept of RAG and demonstrated its effectiveness in improving the performance of large language models on knowledge-intensive natural language processing tasks. Since then, RAG has gained popularity in the field of natural language processing and has been used in various applications to enhance the capabilities of language models.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "88063264-fdef-4bb3-a7c5-3a3eb1a1cff3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\n",
            "\n",
            "\n",
            "Tool Call - Name: arxiv + Query: {\"query\":\"QLoRA in Machine Learning\"}\n",
            "Tool Response: Published: 2023-05-23\n",
            "Title: QLoRA: Efficient Finetuning of Quantized LLMs\n",
            "Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage\n",
            "enough to finetune a 65B parameter model on a single 48GB GPU while preserving\n",
            "full 16-bit finetuning task performance. QLoRA backpropagates gradients through\n",
            "a frozen, 4-bit quantized pretrained language model into Low Rank\n",
            "Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\n",
            "previous openly released models on the Vicuna benchmark, reaching 99.3% of the\n",
            "performance level of ChatGPT while only requiring 24 hours of finetuning on a\n",
            "single GPU. QLoRA introduces a number of innovations to save memory without\n",
            "sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\n",
            "information theoretically optimal for normally distributed weights (b) double\n",
            "quantization to reduce the average memory footprint by quantizing the\n",
            "quantization constants, and (c) paged optimziers to manage memory spikes. We\n",
            "use QLoRA to finetune more than 1,000 models, providing a detailed analysis of\n",
            "instruction following and chatbot performance across 8 instruction datasets,\n",
            "multiple model types (LLaMA, T5), and model scales that would be infeasible to\n",
            "run with regular finetuning (e.g. 33B and 65B parameter models). Our results\n",
            "show that QLoRA finetuning on a small high-quality dataset leads to\n",
            "state-of-the-art results, even when using smaller models than the previous\n",
            "SoTA. We provide a detailed analysis of chatbot performance based on both human\n",
            "and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\n",
            "alternative to human evaluation. Furthermore, we find that current chatbot\n",
            "benchmarks are not trustworthy to accurately evaluate the performance levels of\n",
            "chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\n",
            "ChatGPT. We release all of our models and code, including CUDA kernels for\n",
            "4-bit training.\n",
            "\n",
            "Published: 2023-12-31\n",
            "Title: Viz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\n",
            "Authors: Dipankar Sarkar\n",
            "Summary: This paper aims to introduce and analyze the Viz system in a comprehensive\n",
            "way, a novel system architecture that integrates Quantized Low-Rank Adapters\n",
            "(QLoRA) to fine-tune large language models (LLM) within a legally compliant and\n",
            "resource efficient marketplace. Viz represents a significant contribution to\n",
            "the field of artificial intelligence, particularly in addressing the challenges\n",
            "of computational efficiency, legal compliance, and economic sustainability in\n",
            "the utilization and monetization of LLMs. The paper delineates the scholarly\n",
            "discourse and developments that have informed the creation of Viz, focusing\n",
            "primarily on the advancements in LLM models, copyright issues in AI training\n",
            "(NYT case, 2023), and the evolution of model fine-tuning techniques,\n",
            "particularly low-rank adapters and quantized low-rank adapters, to create a\n",
            "sustainable and economically compliant framework for LLM utilization. The\n",
            "economic model it proposes benefits content creators, AI developers, and\n",
            "end-users, delineating a harmonious integration of technology, economy, and\n",
            "law, offering a comprehensive solution to the complex challenges of today's AI\n",
            "landscape.\n",
            "\n",
            "Published: 2024-02-08\n",
            "Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
            "Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "Summary: The LoRA-finetuning quantization of LLMs has been extensively studied to\n",
            "obtain accurate yet compact LLMs for deployment on resource-constrained\n",
            "hardware. However, existing methods cause the quantized LLM to severely degrade\n",
            "and even fail to benefit from the finetuning of LoRA. This paper proposes a\n",
            "novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\n",
            "through information retention. The proposed IR-QLoRA mainly relies on two\n",
            "technolog\n",
            "\n",
            "Tool Call - Name: tavily_search_results_json + Query: {\"query\":\"Tim Dettmers bio\"}\n",
            "Tool Response: [{'url': 'https://cs.duke.edu/events/accessible-foundation-models-systems-algorithms-and-science', 'content': \"Speaker Bio. Tim Dettmers 's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. These methods ...\"}, {'url': 'https://cs.nyu.edu/dynamic/about/news/colloquium/1344/', 'content': \"Speaker Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning.\"}, {'url': 'https://timdettmers.com/about/', 'content': 'Tim Dettmers, Luke Zettlemoyer. [bib] 2018. Convolutional 2D Knowledge Graph Embeddings, Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel. AAAI2018. 2016. 8-Bit Approximations for Parallelism in Deep Learning, Tim Dettmers. ICLR2016.'}, {'url': 'https://huggingface.co/timdettmers', 'content': '261 followers. ¬∑. 1 following. https://timdettmers.com. tim_dettmers. timdettmers. None yet. Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA. A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using transformers, accelerate and bitsandbytes.'}, {'url': 'https://scholar.google.com/citations?user=lHI3w5kAAAAJ', 'content': 'Tim Dettmers. University of Washington. Verified email at cs.washington.edu - Homepage. Deep Learning Natural Language Processing. Articles Cited by Public access Co-authors. Title. ... A Borzunov, M Ryabinin, T Dettmers, Q Lhoest, L Saulnier, M Diskin, ... NeurIPS 2021 Demonstration, 2022. 9: 2022:'}]\n",
            "\n",
            "Agent Response: QLoRA stands for Quantized Low-Rank Adapters, which is an efficient finetuning approach in machine learning. It reduces memory usage significantly, allowing for the finetuning of large language models on a single GPU while maintaining performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA).\n",
            "\n",
            "One of the technical papers related to QLoRA is titled \"QLoRA: Efficient Finetuning of Quantized LLMs\" authored by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. The paper introduces the QLoRA approach and its innovations in memory-saving techniques for finetuning large language models.\n",
            "\n",
            "The first author of the QLoRA paper is Tim Dettmers. Tim Dettmers' research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. He works on developing novel compression and networking algorithms to enable memory-efficient, fast, and cost-effective deep learning systems.\n",
            "\n",
            "You can find more information about Tim Dettmers on his [personal website](https://timdettmers.com/).\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "#### üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "#### ANSWER:\n",
        "\n",
        "1) It first called the Arxiv tool to search for \"QLoRA in Machine Learning\"\n",
        "2) It then identified \"QLoRA: Efficient Finetuning of Quantized LLMs\" as the QLoRA paper and picked up the name of its first author \"Tim Dettmers\"\n",
        "3) Next, it called the Tavily tool to search for the bio of the first author using the query \"Tim Dettmers bio\"\n",
        "4) Finally it returned a response which described QLoRA, gave the title and authors of the QLoRA paper, and returned information on the author's bio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQmrzYfrm1Dr"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "6eda06b2-0110-44c0-8106-b1280376a2c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I am unable to retrieve search results for RAG at the moment. Would you like me to try searching for something else or provide information on a different topic?'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "## Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "#### üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions.\n",
        "\n",
        "#### ANSWER:\n",
        "\n",
        "The custom dataset is given below (the original dataset from the markdown was used for the actual notebook execution, but my custom data is available as commented code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "    # \"What is the name of the paper that introduced the Transformer architecture in deep learning?\",\n",
        "    # \"Who is the first author of the Attention Is All You Need paper?\",\n",
        "    # \"What is the name of the GPT-3 paper?\",\n",
        "    # \"What is the name of the paper introducing the MMLU benchmark?\",\n",
        "    # \"Who is the first author of the Measuring Massive Multitask Language Understanding paper?\",\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "    # {\"must_mention\" : [\"Attention\", \"Is\", \"All\", \"You\", \"Need\"]},\n",
        "    # {\"must_mention\" : [\"Ashish\", \"Vaswani\"]},\n",
        "    # {\"must_mention\" : [\"Language\", \"Models\", \"are\", \"Few-Shot\", \"Learners\"]},\n",
        "    # {\"must_mention\" : [\"Measuring\", \"Massive\", \"Multitask\", \"Language\", \"Understanding\"]},\n",
        "    # {\"must_mention\" : [\"Dan\", \"Hendrycks\"]},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not\n",
        "\n",
        "#### ANSWER:\n",
        "\n",
        "The `Client.create_examples()` method zips the `questions` object with the `answers` object and creates the association between the two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "## Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "#### ANSWER:\n",
        "Some ways this might be improved are:\n",
        "1) We could update some of the `must_mention` word lists in the `answers` object to be more accurate to the respective questions (for example the \"What is a Retrieval Augmented Generation system?\" answer could also include \"data source\").\n",
        "2) We could change the scoring logic, instead of using `score=all()` which requires all those exact match values to be retuned in the response (this would also require some updates to the answers).\n",
        "3) We could use a full sentence answers in the `answers` object, and then use semantic matching through a method like cosine similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
        "\n",
        "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
        "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "045f1296-2bee-43c8-8b92-b24dc242ff88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - 90ab1674' at:\n",
            "https://smith.langchain.com/o/7fd07b5c-2cc4-5fa5-998d-744414e86be0/datasets/3e2c3f9f-bb84-490c-8404-f300bdeeaba2/compare?selectedSessions=f8976b36-9fb7-459a-8d34-2c34dffdf37c\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - b8c0d350 at:\n",
            "https://smith.langchain.com/o/7fd07b5c-2cc4-5fa5-998d-744414e86be0/datasets/3e2c3f9f-bb84-490c-8404-f300bdeeaba2\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.COT Contextual Accuracy</th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9f806170-bd75-4e26-9b0f-ec6da8982389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.481814</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.408248</td>\n",
              "      <td>0.447214</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.997515</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.647430</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.363078</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.704475</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.942334</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.563606</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        feedback.helpfulness  feedback.COT Contextual Accuracy  \\\n",
              "count               6.000000                          5.000000   \n",
              "unique                   NaN                               NaN   \n",
              "top                      NaN                               NaN   \n",
              "freq                     NaN                               NaN   \n",
              "mean                0.833333                          0.800000   \n",
              "std                 0.408248                          0.447214   \n",
              "min                 0.000000                          0.000000   \n",
              "25%                 1.000000                          1.000000   \n",
              "50%                 1.000000                          1.000000   \n",
              "75%                 1.000000                          1.000000   \n",
              "max                 1.000000                          1.000000   \n",
              "\n",
              "       feedback.must_mention error  execution_time  \\\n",
              "count                      6     0        6.000000   \n",
              "unique                     2     0             NaN   \n",
              "top                     True   NaN             NaN   \n",
              "freq                       3   NaN             NaN   \n",
              "mean                     NaN   NaN        3.481814   \n",
              "std                      NaN   NaN        0.997515   \n",
              "min                      NaN   NaN        1.647430   \n",
              "25%                      NaN   NaN        3.363078   \n",
              "50%                      NaN   NaN        3.704475   \n",
              "75%                      NaN   NaN        3.942334   \n",
              "max                      NaN   NaN        4.563606   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      6  \n",
              "unique                                     6  \n",
              "top     9f806170-bd75-4e26-9b0f-ec6da8982389  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - 90ab1674',\n",
              " 'results': {'bbd7631e-b2ea-4d11-8b07-27c8bc28cfe5': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a direct answer to the question asked in the input, which is about the optimizer used in QLoRA. The answer is specific and includes additional information about how to access the optimizer, which could be useful for someone trying to use or understand QLoRA. \\n\\nTherefore, the submission can be considered helpful, insightful, and appropriate. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0685864a-fccb-45e3-8310-8bfe38c1a926'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=None, value=None, comment='The context provided does not give any information about QLoRA or its optimizer. The student\\'s answer mentions a \"paged optimizer\" and provides additional details. However, without any information in the context to confirm or refute this, it is impossible to determine the accuracy of the student\\'s answer. \\nGRADE: UNDETERMINABLE', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('041360ca-2282-486f-b281-77a06ccb7ebe'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('9216fa41-bee0-4cb6-b423-0b5da98d9a45'), target_run_id=None)],\n",
              "   'execution_time': 3.9267,\n",
              "   'run_id': '9f806170-bd75-4e26-9b0f-ec6da8982389',\n",
              "   'output': 'The optimizer used in QLoRA is the \"paged optimizer\" which can be accessed with the argument \"--optim paged_adamw_32bit\".',\n",
              "   'reference': {'must_mention': ['paged', 'optimizer']}},\n",
              "  'a408c50e-ac13-47d6-a050-e908602942b8': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a direct answer to the question, stating that the data type created in the QLoRA paper is \"4-bit NormalFloat (NF4)\". This is helpful as it directly answers the question.\\n\\nAdditionally, the submission provides extra information about the \"4-bit NormalFloat (NF4)\" data type, stating that it is a new data type that is information theoretically optimal for normally distributed weights. This is insightful as it provides additional context about the data type, which could be useful for someone trying to understand the QLoRA paper.\\n\\nThe submission is also appropriate as it stays on topic and answers the question in a professional and concise manner.\\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('d7997e25-c6b7-4e16-b682-92efe2c2e805'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment='The student\\'s answer matches the context given. The student correctly identified the data type created in the QLoRA paper as \"4-bit NormalFloat (NF4)\". The additional information provided by the student about the data type being information theoretically optimal for normally distributed weights does not conflict with the context provided.\\nGRADE: CORRECT', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0e2943ae-15ea-41f3-927d-0010ea0bb775'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('f8657cc0-7040-4da1-9843-e4f6a2b0c6b6'), target_run_id=None)],\n",
              "   'execution_time': 3.48225,\n",
              "   'run_id': '17816a11-1a71-48cb-8eb9-e6859a908a04',\n",
              "   'output': 'The data type created in the QLoRA paper is called \"4-bit NormalFloat (NF4)\", which is a new data type that is information theoretically optimal for normally distributed weights.',\n",
              "   'reference': {'must_mention': ['NF4', 'NormalFloat']}},\n",
              "  'e09acc6d-0df6-474d-8e04-a72a2ad57ed8': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nThe submission provides a detailed explanation of what a Retrieval Augmented Generation (RAG) system is. It explains the process of how a RAG system works, starting from retrieving relevant information to generating a response. \\n\\nThe submission also explains the purpose and benefits of a RAG system, stating that it aims to improve the quality and relevance of generated text by incorporating information retrieval techniques. \\n\\nThe submission is insightful as it provides a comprehensive understanding of the topic. It is also appropriate as it directly answers the given input.\\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('792d90aa-de8b-4228-b2dc-962916a5255d'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is detailed and comprehensive. They correctly define a Retrieval Augmented Generation (RAG) system as a type of natural language processing model that combines information retrieval and text generation. They also correctly explain the process of how a RAG system works, its purpose, and its advantages over traditional text generation models. The student's answer does not conflict with the context provided.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9ff52216-2f9a-45c4-b211-f118dfd740ca'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('2f89982a-6f90-45eb-b926-bc4aae7798bc'), target_run_id=None)],\n",
              "   'execution_time': 3.323354,\n",
              "   'run_id': 'e903f5d5-8456-466c-8d52-5c690464d3fe',\n",
              "   'output': 'A Retrieval Augmented Generation (RAG) system is a type of natural language processing (NLP) model that combines the capabilities of information retrieval and text generation. In a RAG system, the model first retrieves relevant information from a large corpus of text based on a given query or prompt. This retrieved information is then used to generate a coherent and informative response.\\n\\nRAG systems are designed to improve the quality and relevance of generated text by incorporating information retrieval techniques. By leveraging pre-existing knowledge from a large dataset, RAG systems can generate more accurate and contextually relevant responses compared to traditional text generation models.\\n\\nOverall, RAG systems aim to enhance the performance of text generation tasks by integrating information retrieval into the generation process, leading to more informative and coherent outputs.',\n",
              "   'reference': {'must_mention': ['ground', 'context']}},\n",
              "  '30be5b3c-24e8-48f1-b262-2f8b04932ca8': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission is supposed to be helpful, insightful, and appropriate. \\n\\nLooking at the input, the question asked is about the authorship of the QLoRA paper. \\n\\nThe submission provided a list of authors, but it refers to a different paper, the QDyLoRA paper, not the QLoRA paper as asked in the input. \\n\\nThis means that the submission is not appropriate to the question asked, and therefore, it is not helpful. \\n\\nSo, the submission does not meet the criterion. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ae0ebeb7-a33b-49ff-b459-3c03f7e22e88'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=\"The student's answer does not match the context provided. The context states that the authors of the QLoRA paper are 'Tim' and 'Dettmers', but the student has listed a completely different set of authors. Therefore, the student's answer is incorrect.\\nGRADE: INCORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('507b47a0-5c8d-443c-85c9-e4fee8a7aab0'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('fe932199-b3fd-4ca0-93d4-18fe48522d77'), target_run_id=None)],\n",
              "   'execution_time': 3.947546,\n",
              "   'run_id': '5429313d-f230-4f71-8686-3d6d60b2ce70',\n",
              "   'output': 'The QDyLoRA paper was authored by Hossein Rajabzadeh, Mojtaba Valipour, Tianshu Zhu, Marzieh Tahaei, Hyock Ju Kwon, Ali Ghodsi, Boxing Chen, and Mehdi Rezagholizadeh.',\n",
              "   'reference': {'must_mention': ['Tim', 'Dettmers']}},\n",
              "  '0bc4ca5a-43ab-4f45-ac6f-0788a28e9f30': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nThe submission provides a clear answer to the question, stating that TensorFlow is the most popular deep learning framework. \\n\\nIn addition to answering the question, the submission also provides additional information about TensorFlow, such as its open-source nature, its developer, and its uses. This information is helpful for someone who may not be familiar with TensorFlow.\\n\\nThe submission also mentions other popular deep learning frameworks, which provides a broader context and could be helpful for someone looking for alternatives to TensorFlow.\\n\\nBased on these points, the submission can be considered helpful, insightful, and appropriate.\\n\\nTherefore, the submission meets the criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('19ba6a97-0922-4d1b-82ad-7e3d4ba8a308'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer states that TensorFlow is the most popular deep learning framework. This aligns with the context provided, which lists TensorFlow as one of the popular deep learning frameworks. The student also provides additional information about TensorFlow, which does not contradict the context. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('71f6c9ff-ef7d-4fbb-8143-4901799cbcbd'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('685a7f79-e760-4322-a633-6a782e1a3598'), target_run_id=None)],\n",
              "   'execution_time': 1.64743,\n",
              "   'run_id': '6a851e13-b25f-44d8-92b8-317a7808d039',\n",
              "   'output': 'The most popular deep learning framework is TensorFlow. TensorFlow is an open-source machine learning framework developed by Google that is widely used for building and training deep learning models. It provides a comprehensive ecosystem of tools, libraries, and community support for deep learning research and applications. Other popular deep learning frameworks include PyTorch, Keras, and Caffe.',\n",
              "   'reference': {'must_mention': ['PyTorch', 'TensorFlow']}},\n",
              "  '3ddc6811-db42-42ef-a8d5-13597eb3a678': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, it provides a detailed explanation of the improvements that the LoRa system makes. It lists six key improvements, each with a brief explanation. This makes the submission helpful as it provides the necessary information to answer the question.\\n\\nThe submission is also insightful. It not only lists the improvements but also explains why these improvements are significant. For example, it explains that the low power consumption of LoRa devices makes them ideal for battery-operated devices and applications that require long battery life.\\n\\nThe submission is appropriate as it directly answers the question asked in the input. It stays on topic and provides relevant information.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b1bd7c46-2b68-47a3-ba44-33f648141150'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer provides a detailed explanation of the significant improvements that the LoRa system makes. The student mentions long range, low power consumption, low cost, scalability, secure communication, and flexibility as key improvements. However, the context provided for grading the answer is ['reduce', 'parameters']. This context is vague and does not provide specific parameters that the LoRa system is supposed to improve. Therefore, based on the context provided, it is difficult to accurately grade the student's answer. However, the student's answer is factually correct and provides a comprehensive overview of the improvements made by the LoRa system. \\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1d636e2d-867e-4c60-91cd-c8e4b36c0ba0'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('8f362e37-6d83-4ffd-bbd5-14dc176af8db'), target_run_id=None)],\n",
              "   'execution_time': 4.563606,\n",
              "   'run_id': 'a22dde7d-3e71-469a-9d1d-4c4502de32b9',\n",
              "   'output': 'The LoRa (Long Range) system offers several significant improvements compared to traditional wireless communication systems. Some of the key improvements include:\\n\\n1. Long Range: LoRa technology enables long-range communication, allowing devices to transmit data over several kilometers in rural areas and up to several hundred meters in urban environments.\\n\\n2. Low Power Consumption: LoRa devices are designed to operate on low power, making them ideal for battery-operated devices and applications that require long battery life.\\n\\n3. Low Cost: LoRa technology is cost-effective, making it suitable for a wide range of applications, including smart cities, industrial automation, agriculture, and more.\\n\\n4. Scalability: The LoRa system is highly scalable, allowing for the deployment of a large number of devices within a network without compromising performance.\\n\\n5. Secure Communication: LoRa technology provides secure communication through encryption and authentication mechanisms, ensuring the confidentiality and integrity of data transmission.\\n\\n6. Flexibility: LoRa supports bi-directional communication, enabling devices to both send and receive data, making it versatile for various IoT applications.\\n\\nOverall, the LoRa system offers a reliable, cost-effective, and energy-efficient solution for long-range wireless communication, making it well-suited for IoT applications and smart city deployments.',\n",
              "   'reference': {'must_mention': ['reduce', 'parameters']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "## Part 2: Helpfulness Check\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add a custom node and conditional edge to determine if the response was helpful enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC8t-4FISCEh"
      },
      "source": [
        "We're going to add a custom helpfulness check here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZV_PxI5zNY7f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def check_helpfulness(state):\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  # Added by Yohan\n",
        "  print(\"Helpfulness Check: State Messages Length =\", len(state[\"messages\"]))\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    print(\"Helpful!\\n\")\n",
        "    return \"end\"\n",
        "  else:\n",
        "    print(\"Not helpful!\\n\")\n",
        "    return \"continue\"\n",
        "\n",
        "def dummy_node(state):\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "#### üèóÔ∏è Activity #3:\n",
        "\n",
        "Please write what is happening in our `check_helpfulness` function!\n",
        "\n",
        "#### ANSWER:\n",
        "\n",
        "In the `check_helpfulness` function, the folowing steps are executed:\n",
        "1) We first create a prompt template from the `initial_query` and the `final_response` messages, and also instruct the model to indicate the helpfulness of the final response as it relates to the initial query\n",
        "2) We then instantiate our GPT-4 evaluator model and create a chain using the prompt template, model and an output parser\n",
        "3) Finally we invoke the chain with the `initial_query` and `final_response` messages, and then pass either \"end\" or \"continue\" depending on whether the model was helpful or not. This helps the agent graph decide whether to continue the cycle or terminate it and return the final response to the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", call_tool)\n",
        "graph_with_helpfulness_check.add_node(\"passthrough\", dummy_node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : \"passthrough\"\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"passthrough\",\n",
        "    # should_continue,\n",
        "    # Added by Yohan\n",
        "    check_helpfulness,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OWOPweuSiHc"
      },
      "source": [
        "Let's compile and test!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "7e2ea696-ba42-468e-a842-2c585f246c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helpfulness Check: State Messages Length = 8\n",
            "Helpful!\n",
            "Initial Query: Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\n",
            "\n",
            "\n",
            "Tool Call - Name: tavily_search_results_json + Query: {\"query\":\"LoRA in machine learning\"}\n",
            "Tool Response: [{'url': 'https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578', 'content': 'DBS Bank | Youtube: https://www.youtube.com/channel/UCQoNosQTIxiMTL9C-gvFdjA\\nMore from Mehul Gupta and Data Science in your pocket\\nMehul Gupta\\nin\\nData Science in your pocket\\nRecommendation Systems using Langchain and LLMs with codes\\nusing RAG framework and chains\\n--\\n3\\nMehul Gupta\\nin\\nData Science in your pocket\\nLangchain tutorials for newbies\\nLangchain use cases with demo explained\\n--\\n2\\nMehul Gupta\\nin\\nData Science in your pocket\\nWhat are Vector Databases and How Langchain uses Vector DBs\\nwith codes and examples\\n--\\n1\\nMehul Gupta\\nin\\nData Science in your pocket\\nBest AI-Agents you should know\\nHuggingGPT, DemoGPT, AutoGPT and more\\n--\\nRecommended from Medium\\nBijit Ghosh\\nAdvanced Techniques for Fine-Tuning LLMs\\nIntroduction\\n--\\nGaurav Garg\\nin\\nGoPenAI\\nUnderstanding LLM Fine Tuning\\u200a‚Äî\\u200aA Complete Guide for Everyone\\nThe Rise of Large Language Models and Fine Tuning\\n--\\nLists\\nPredictive Modeling w/ Python\\nChatGPT prompts\\nChatGPT\\nAI Regulation\\nDhanoop Karunakaran\\nin\\nIntro to Artificial Intelligence\\nFine-tuning Large Language Models series: Part1\\u200a‚Äî\\u200aInternal mechanism of LLMs\\n Akriti Upadhyay\\nEmploying QdrantDB to conduct advanced similarity searches for image data\\nImplementing Image Similarity Search using Qdrant\\n--\\n1\\nAbhinav Kimothi\\nin\\nMLearning.ai\\nWhat is a fine-tuned LLM?\\nFine-tuning large language models (LLMs) has become a powerful technique for achieving impressive performance in various natural language‚Ä¶\\n--\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams Have a look below\\nWe will start off with pip installing libraries\\nNext, import the important functions required\\nNext, let‚Äôs load the training and test dataset alongside the LLM to be fine-tuned with its tokenizer\\nNext, using the Input and Output, we will create a prompt template which is a requirement by the SFTTrainer we will be using later\\nNow is the time we set the trainer for LoRA\\n LoRA for Fine-Tuning LLMs explained with codes and example\\nHow to fine-tune your LLMs faster using LoRA\\nMehul Gupta\\nFollow\\nData Science in your pocket\\n--\\nListen\\nShare\\nThis entire year in AI space has been revolutionary because of the advancements in Gen-AI especially the incoming of LLMs. Once done, login into huggingface-hub using the WRITE token in the Jupyter Notebook and push your model using the below code\\nNow, upload the model\\nNow, once uploaded, again use the notebook login but using the READ token this time\\nNow run the below code for inferencing with your private model\\nSee the output for yourself\\nWith this, we will be wrapping up this very long post.'}, {'url': 'https://machinelearningmastery.com/using-lora-in-stable-diffusion/', 'content': 'LoRA, or Low-Rank Adaptation, is a lightweight training technique used for fine-tuning Large Language and Stable Diffusion Models without needing full model training. Full fine-tuning of larger models (consisting of billions of parameters) is inherently expensive and time-consuming. LoRA works by adding a smaller number of new weights to the ...'}, {'url': 'https://towardsdatascience.com/lora-intuitively-and-exhaustively-explained-e944a6bff46b', 'content': 'More from Daniel Warfield and Towards Data Science\\nDaniel Warfield\\nin\\nTowards Data Science\\nTransformers\\u200a‚Äî\\u200aIntuitively and Exhaustively Explained\\nExploring the modern wave of machine learning: taking apart the transformer step by step\\n--\\n14\\nRahul Nayak\\nin\\nTowards Data Science\\nHow to Convert Any Text Into a Graph of Concepts\\nA method to convert any text corpus into a Knowledge Graph using Mistral 7B.\\n--\\n35\\nMarco Peixeiro\\nin\\nTowards Data Science\\nTimeGPT: The First Foundation Model for Time Series Forecasting\\nExplore the first generative pre-trained forecasting model and apply it in a project with Python\\n--\\n22\\nDaniel Warfield\\nin\\nTowards Data Science\\nConvolutional Networks\\u200a‚Äî\\u200aIntuitively and Exhaustively Explained\\nUnpacking a cornerstone modeling strategy\\n--\\n3\\nRecommended from Medium\\nRahul Nayak\\nin\\nTowards Data Science\\nHow to Convert Any Text Into a Graph of Concepts\\nA method to convert any text corpus into a Knowledge Graph using Mistral 7B.\\n--\\n35\\nAlden Do Rosario\\nLangchain is NOT for production use. From MLOps to mounting‚Ä¶\\n--\\n20\\nLists\\nPredictive Modeling w/ Python\\nPractical Guides to Machine Learning\\nNatural Language Processing\\nNew_Reading_List\\nPaul Rose\\nI Found A Very Profitable AI Side Hustle\\nAnd it‚Äôs perfect for beginners\\n--\\n192\\nDevansh\\nin\\nDataDrivenInvestor\\nWhy Elon Musks AI Model Grok is the future of LLMs\\nWhy Big Tech Companies should copy the Grok approach ASAP\\n--\\n11\\nThe Pareto Investor\\nNASA Just Shut Down Quantum Computer After Something Insane Happened!\\n Houston, We Have a Problem!\\n--\\n136\\nGathnex\\nMistral-7B Fine-Tuning: A Step-by-Step Guide\\nIntroducing Mistral 7B: The Powerhouse of Language Models\\n--\\n6\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams Member-only story\\nNatural Language Processing | Machine Learning\\nLoRA ‚Äî Intuitively and Exhaustively Explained\\nExploring the modern wave of machine learning with cutting edge fine tuning\\nDaniel Warfield\\nFollow\\nTowards Data Science\\n--\\n7\\nShare\\nFine tuning is the process of tailoring a machine learning model to a specific application, which can be vital in achieving consistent and high quality performance. What, and Why, is Fine Tuning?\\n--\\n--\\n7\\nWritten by Daniel Warfield\\nTowards Data Science\\nData Scientist, Educator, Artist, Writer.\\n'}, {'url': 'https://arxiv.org/abs/2106.09685', 'content': 'An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is ...'}, {'url': 'https://www.datacamp.com/tutorial/mastering-low-rank-adaptation-lora-enhancing-large-language-models-for-efficient-adaptation', 'content': 'Natassha Selvaraj\\n8 min\\nWhat Fortune 1000 Executives Believe about Data & AI in 2024 with Randy Bean, Innovation Fellow, Data Strategy, Wavestone\\nRichie Cotton\\n46 min\\nData Security in the Age of AI with Bart Vandekerckhove, Co-founder at Raito\\nRichie Cotton\\n46 min\\nBuilding Trustworthy AI with Alexandra Ebert, Chief Trust Officer at MOSTLY AI\\nRichie Cotton\\n50 min\\nHow Transformers Work: A Detailed Exploration of Transformer Architecture\\nJosep Ferrer\\n15 min\\nCross-Entropy Loss Function in Machine Learning: Latest news about our products and team\\nDiscover content by tools and technology\\nDiscover content by data science topics\\nMastering Low-Rank Adaptation (LoRA): Enhancing Large Language Models for Efficient Adaptation\\nLoRA: Low-Rank Adaptation of Large Language Models\\nThe field of machine learning and natural language processing (NLP) has witnessed a remarkable advancement with the introduction of Large Language Models (LLMs) such as GPT, LLaMa, Claude 2, etc. Benefits of pre-fix tuning:\\nLoRA and prefix tuning can be combined within the PEFT (Parameter Efficient Fine-Tuning) framework:\\nExample of LoRA implementation using Loralib library in Python\\nTo implement LoRA, you can use the Loralib library by Microsoft. What Fortune 1000 Executives Believe about Data & AI in 2024 with Randy Bean, Innovation Fellow, Data Strategy, Wavestone\\nData Security in the Age of AI with Bart Vandekerckhove, Co-founder at Raito\\nBuilding Trustworthy AI with Alexandra Ebert, Chief Trust Officer at MOSTLY AI\\nHow Transformers Work: A Detailed Exploration of Transformer Architecture\\nCross-Entropy Loss Function in Machine Learning: Advantages of LoRA\\nThere are several advantages that come with using LoRA for fine-tuning:\\n1. Efficiency in training and deployment\\nLoRA reduces the computational burden, allowing faster adaptation of models.'}]\n",
            "\n",
            "Tool Call - Name: tavily_search_results_json + Query: {\"query\":\"Tim Dettmers\"}\n",
            "Tool Response: [{'url': 'https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/', 'content': 'Dear Tim Dettmers, Thank you very much for this blog. This information is really useful for upcoming deep learning project. In my work place we are developing a server kind on of system to run three deep learning projects. To run all the models in a concurrent manner for 24√ó7, I need nearly 100 GB GTX 2080 Ti GPU. To maintain this GPU what ...'}, {'url': 'https://github.com/TimDettmers', 'content': 'You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window.'}, {'url': 'https://scholar.google.com/citations?user=lHI3w5kAAAAJ', 'content': 'Tim Dettmers. University of Washington. Verified email at cs.washington.edu - Homepage. Deep Learning Natural Language Processing. Articles Cited by Public access Co-authors. Title. ... A Borzunov, M Ryabinin, T Dettmers, Q Lhoest, L Saulnier, M Diskin, ... NeurIPS 2021 Demonstration, 2022. 9: 2022:'}, {'url': 'https://timdettmers.com/category/deep-learning/', 'content': 'Which GPU (s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning. 2023-01-30 by Tim Dettmers 1,664 Comments. Deep learning is a field with intense computational requirements, and your choice of GPU will fundamentally determine your deep learning experience. But what features are important if you want to buy a new ...'}, {'url': 'https://timdettmers.com/', 'content': 'Tim Dettmers is a researcher and educator in deep learning and machine learning. His blog covers topics such as GPU choice, grad school selection, creativity, sparse networks, and TPUs vs GPUs for transformers.'}]\n",
            "\n",
            "Tool Call - Name: tavily_search_results_json + Query: {\"query\":\"Attention in machine learning\"}\n",
            "Tool Response: [{'url': 'https://towardsdatascience.com/introduction-to-attention-mechanism-8d044442a29', 'content': \"That‚Äôs why you‚Äôre not using just a dot product, but a scaled dot product, that way our new formula looks like\\nIf you‚Äôre a having problem understanding why dot product creates a large number with high dimensional vectors please check 3Blue1Brown‚Äôs Youtube video on the subject\\nAdditionally, we want to be able to use more than one query vector q. It was great to have a single query vector for each timestamp of the decoder but it can be a lot simpler when we use all of them at the same time, so we change our vector to vectors Q (Shape NQ\\u200b√óDQ\\u200b). References:\\nOriginally published at https://erdem.pl.\\n--\\n--\\nWritten by Kemal Erdem (burnpiro)\\nTowards Data Science\\nML Engineer, Javascript Architect, Consultant, MTB lover\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams The reason was to match the colors used in The Illustrated Transformer blog post.\\nConclusions\\nFrom this point onwards we can use the Self-Attention Layer to create a Transformer but this article is too long already. In the case of the token woman, both people on the image save similar attention weights but that's still ok because the model could decide which one is the subject and how to name that person.\\n Now we have to pass the same grid and s0\\u200b to the alignment function to calculate the corresponding alignment score for each value of the grid\\nThat gives us alignment scores for t=1 timestep.\"}, {'url': 'https://machinelearningmastery.com/the-attention-mechanism-from-scratch/', 'content': 'The General Attention Mechanism\\nThe general attention mechanism makes use of three main components, namely the queries, $\\\\mathbf{Q}$, the keys, $\\\\mathbf{K}$, and the values, $\\\\mathbf{V}$.\\nIf you had to compare these three components to the attention mechanism as proposed by Bahdanau et al., then the query would be analogous to the previous decoder output, $\\\\mathbf{s}_{t-1}$, while the values would be analogous to the encoded inputs, $\\\\mathbf{h}_i$. In the Bahdanau attention mechanism, the keys and values are the same vector.\\n But if we have thought for example the first row of the score matrix as:\\n[ word1/word1 word1/word2 word1/word3 word1/word4 ]\\nthen the elements of the attention vector (a row of the attention matrix) might be interpreted as somewhat a coded version of the row of the score matrix since in the product between two matrix we do rows*columns product.\\n In the latter case, if I get, for example, word 3, then the first element of the corresponding attention vector (row) is related to the first word, the second element to the second, and the third element to the fourth word ?.\\n Note\\xa0that Bahdanau et al.‚Äôs attention mechanism is divided into the step-by-step computations of the alignment scores, the weights, and the context vector:\\n$$e_{t,i} = a(\\\\mathbf{s}_{t-1}, \\\\mathbf{h}_i)$$\\n$$\\\\alpha_{t,i} = \\\\text{softmax}(e_{t,i})$$\\n$$\\\\mathbf{c}_t = \\\\sum_{i=1}^T \\\\alpha_{t,i} \\\\mathbf{h}_i$$\\nBahdanau et al. implemented an RNN for both the encoder and decoder.\\n The idea behind the attention mechanism was to permit the decoder to utilize the most relevant parts of the input sequence in a flexible manner, by a weighted combination of all the encoded input vectors, with the most relevant vectors being attributed the highest weights.\\n'}, {'url': 'https://machinelearningmastery.com/what-is-attention/', 'content': 'The human brain does so by relying on attention, such that it dynamically stores in memory the information that the human subject most pays attention to.\\nAttention in Machine Learning\\nImplementing the attention mechanism in artificial neural networks does not necessarily track the biological and psychological mechanisms of the human brain. A list of these vectors (the second component of the attention-based system above), together with the decoder‚Äôs previous hidden states, will be exploited by the attention mechanism to dynamically highlight which of the input information will be used to generate the output.\\n LinkedIn |\\nTwitter |\\nFacebook |\\nNewsletter |\\nRSS\\nPrivacy |\\nDisclaimer |\\nTerms |\\nContact |\\nSitemap |\\nSearch Instead, it is the ability to dynamically highlight and use the salient parts of the information at hand‚Äîin a similar manner as it does in the human brain‚Äîthat makes attention such an attractive concept in machine learning.\\n At each time step, the attention mechanism then takes the previous hidden state of the decoder and the list of encoded vectors, using them to generate unnormalized score values that indicate how well the elements of the input sequence align with the current output.'}, {'url': 'https://www.geeksforgeeks.org/ml-attention-mechanism/', 'content': 'We can observe 3 sub-parts or components of the Attention Mechanism architecture :\\nConsider the following Encoder-Decoder architecture with Attention.\\nEncoder-Decoder with Attention\\nEncoder:\\nThe encoder applies recurrent neural networks (RNNs) or transformer-based models to iteratively process the input sequence. Data Structures and Algorithms\\nML & Data Science\\nWeb Development\\nLanguages\\nInterview Corner\\nCS Subjects\\nJobs\\nPractice\\nContests\\nML ‚Äì Attention mechanism\\nLet‚Äôs take a look at hearing and a case study of selective attention in the context of a crowded cocktail party. The context vector is fed into the decoder along with the current hidden state of the decoder in order to predict the next token in the output sequence. The attention mechanism can be represented mathematically as follows:\\nattention\\nThe feed-forward network is responsible for transforming the target hidden state into a representation that is compatible with the attention mechanism. This architecture allows the model to focus on different parts of the input sequence during the translation process, improving the alignment and quality of the translations.'}, {'url': 'https://en.wikipedia.org/wiki/Attention_(machine_learning)', 'content': 'Note that the context vector for \"that\" does not rely on context vectors for the other words; therefore the context vectors of all words can be calculated using the whole matrix X, which includes all the word embeddings, instead of a single word\\'s embedding vector x in the formula above, thus parallelizing the calculations. More correctly, we should take the transpose of the context vector and use the column-wise softmax, resulting in the more correct form\\nThe query vector is compared (via dot product) with each word in the keys. The structure of the input data is captured in the Qw and Kw weights, and the Vw weights express that structure in terms of more meaningful features for the task being trained for.\\n On the first pass through the decoder, 94% of the attention weight is on the first English word \"I\", so the network offers the word \"je\".\\n Multiplying this against the value matrix effectively amplifies the signal for the most important words in the sentence and diminishes the signal for less important words.[9]\\n'}]\n",
            "\n",
            "Agent Response: ### LoRA in Machine Learning:\n",
            "LoRA, or Low-Rank Adaptation, is a lightweight training technique used for fine-tuning Large Language and Stable Diffusion Models without needing full model training. It works by adding a smaller number of new weights to the model, allowing for faster adaptation of models without the computational burden of full fine-tuning. LoRA is particularly useful for fine-tuning large language models efficiently. [Read more](https://machinelearningmastery.com/using-lora-in-stable-diffusion/)\n",
            "\n",
            "### Tim Dettmers:\n",
            "Tim Dettmers is a researcher and educator in deep learning and machine learning. He covers topics such as GPU choice, grad school selection, creativity, sparse networks, and TPUs vs GPUs for transformers. He provides valuable insights and advice in the field of deep learning. [Learn more](https://timdettmers.com/)\n",
            "\n",
            "### Attention in Machine Learning:\n",
            "Attention in machine learning refers to the ability of a model to dynamically highlight and use the salient parts of the information at hand. It allows the model to focus on different parts of the input sequence during the processing, improving alignment and quality of translations. Attention mechanisms are inspired by the human brain's ability to dynamically store and utilize information. [Explore more](https://machinelearningmastery.com/what-is-attention/)\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
